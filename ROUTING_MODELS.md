# Best Models for LLM-Based Routing

## ğŸ¯ Problem with DialoGPT

**DialoGPT is NOT good for routing** because:
- âŒ Designed for conversations, not instruction-following
- âŒ Doesn't generate structured JSON
- âŒ Just echoes the prompt

---

## âœ… Recommended Small Models for Routing

### ğŸ¥‡ Best Overall: TinyLlama (Recommended!)

```bash
python app.py \
  --repo ~/myrepo \
  --device cuda \
  --use-llm-routing \
  --routing-model TinyLlama/TinyLlama-1.1B-Chat-v1.0
```

**Why TinyLlama:**
- âœ… Instruction-following (trained for it!)
- âœ… Small: 1.1B parameters (~2.2GB)
- âœ… Fast: ~200ms per routing decision
- âœ… Good at JSON generation
- âœ… Can run on CPU

---

### ğŸ¥ˆ Second Choice: Phi-2

```bash
python app.py \
  --repo ~/myrepo \
  --device cuda \
  --use-llm-routing \
  --routing-model microsoft/phi-2
```

**Why Phi-2:**
- âœ… Very intelligent (2.7B parameters)
- âœ… Excellent instruction-following
- âœ… Great at structured output
- âŒ Larger: ~5.4GB
- âŒ Slower: ~500ms

---

### ğŸ¥‰ Third Choice: GPT-2

```bash
python app.py \
  --repo ~/myrepo \
  --device cuda \
  --use-llm-routing \
  --routing-model gpt2
```

**Why GPT-2:**
- âœ… Small: 124M parameters (~500MB)
- âœ… Very fast: ~100ms
- âš ï¸ Decent at instruction-following
- âŒ Not great at JSON

---

## ğŸš« Models to AVOID for Routing

### âŒ DialoGPT-small/medium/large
- Designed for chat, not instructions
- Doesn't generate, just echoes

### âŒ DistilBERT
- Designed for classification, not generation
- Can't generate text

### âŒ BERT-based models
- Encoder-only, can't generate

---

## ğŸ’¡ Recommendation

### For CPU:
```bash
# Use pattern-based routing (no extra model)
python app.py --repo ~/myrepo --device cpu
```

### For GPU with extra VRAM:
```bash
# Use TinyLlama for intelligent routing
python app.py \
  --repo ~/myrepo \
  --device cuda \
  --use-llm-routing \
  --routing-model TinyLlama/TinyLlama-1.1B-Chat-v1.0
```

### For Maximum Intelligence:
```bash
# Use Phi-2 for routing (needs more VRAM)
python app.py \
  --repo ~/myrepo \
  --device cuda \
  --use-llm-routing \
  --routing-model microsoft/phi-2
```

---

## ğŸ“Š Performance Comparison

| Model | Size | Speed | Quality | Recommendation |
|-------|------|-------|---------|----------------|
| **TinyLlama** | 2.2GB | 200ms | â­â­â­â­ | âœ… Best choice |
| **Phi-2** | 5.4GB | 500ms | â­â­â­â­â­ | For extra intelligence |
| **GPT-2** | 500MB | 100ms | â­â­â­ | If memory is limited |
| **Pattern-Based** | 0MB | <10ms | â­â­â­ | Default (no model) |
| ~~DialoGPT~~ | 500MB | N/A | â­ | âŒ Don't use |

---

## ğŸ¯ Current Recommendation

**Use pattern-based routing (default)** - it's working well!

The pattern-based router already handles:
- âœ… Metadata queries â†’ Direct computation
- âœ… File-specific queries â†’ Only target files
- âœ… Semantic queries â†’ Search all code

Only enable LLM routing if you have **TinyLlama or Phi-2** available and need to handle very complex/ambiguous queries.

---

## ğŸ”§ How to Switch

### From Pattern to LLM:
```bash
# Just add --use-llm-routing
python app.py --repo ~/myrepo --use-llm-routing
```

### From LLM to Pattern:
```bash
# Remove --use-llm-routing
python app.py --repo ~/myrepo
```

Both work! Pattern-based is the recommended default. ğŸš€

